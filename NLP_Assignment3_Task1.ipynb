{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1lNh7gtvC9iZcyR_0jDFPEzY-8JWOlmdE","authorship_tag":"ABX9TyMhmUsSS2iz3okQdnxAlWJq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yuCHZ__v6ouR","executionInfo":{"status":"ok","timestamp":1688423497994,"user_tz":240,"elapsed":2564,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"outputs":[],"source":["import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk import bigrams\n","from nltk.probability import FreqDist, ConditionalFreqDist\n","from nltk import word_tokenize, sent_tokenize\n","from gensim.models import Word2Vec"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ep3qLNNI7cmm","executionInfo":{"status":"ok","timestamp":1688423504751,"user_tz":240,"elapsed":870,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"d1623915-84dd-4152-c5cf-314897e90d66"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YiH0s2D7cqj","executionInfo":{"status":"ok","timestamp":1688423513872,"user_tz":240,"elapsed":7830,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"f594bb82-9edb-42bb-a1d6-7ce489213cf1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# **Relative Frequency approach**"],"metadata":{"id":"-QP9qXqT3AiL"}},{"cell_type":"code","source":["import string\n","import re\n","import nltk\n","from nltk import FreqDist, bigrams\n","import math\n","\n","def preprocess_text(file_path):\n","    with open(file_path, 'r') as file:\n","        letters = file.read().lower()\n","        letters = letters.translate(str.maketrans('', '', string.punctuation))\n","    return letters\n","\n","def calculate_relative_frequency_perplexity(cfdist, fdist_uni, tokens, smoothing_factor=0.01):\n","    num_words = len(tokens)\n","    vocab_size = len(fdist_uni)\n","    log_sum = 0\n","\n","    for i in range(num_words - 1):\n","        current_word = tokens[i]\n","        next_word = tokens[i + 1]\n","\n","        count_current_word = fdist_uni[current_word] + smoothing_factor * vocab_size\n","        count_bigram = cfdist[current_word][next_word] + smoothing_factor\n","\n","        relative_frequency = count_bigram / count_current_word\n","\n","        if relative_frequency > 0:\n","            log_sum += math.log2(relative_frequency)\n","\n","    perplexity = 2 ** (-log_sum / (num_words - 1))\n","    return perplexity\n","\n","# Preprocess the Warren Buffet's letters\n","file_path = '/content/drive/My Drive/WarrenBuffet.txt'\n","letters = preprocess_text(file_path)\n","\n","# Tokenize the text into words\n","words = nltk.word_tokenize(letters)\n","\n","# Create a list of bigrams\n","word_bigrams = list(bigrams(words))\n","\n","# Calculate the frequency distribution of bigrams\n","freq_dist = FreqDist(word_bigrams)\n","\n","print(freq_dist.most_common(5))\n","\n","# Calculate the relative frequency approach perplexity\n","cfdist = nltk.ConditionalFreqDist(word_bigrams)\n","fdist_uni = FreqDist(words)\n","tokens = words\n","\n","rf_preplexity = calculate_relative_frequency_perplexity(cfdist, fdist_uni, tokens)\n","print(f\"Relative Frequency Approach Perplexity: {rf_preplexity}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AD63nNCa7cx3","executionInfo":{"status":"ok","timestamp":1688423539439,"user_tz":240,"elapsed":410,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"e624788d-d945-49fd-d0c5-0c164ad8d34a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[(('of', 'the'), 223), (('in', 'the'), 210), (('and', 'i'), 116), (('will', 'be'), 113), (('we', 'will'), 105)]\n","Relative Frequency Approach Perplexity: 78.53120031377827\n"]}]},{"cell_type":"code","source":["len(fdist_uni)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJdTz_wD7c1W","executionInfo":{"status":"ok","timestamp":1688423542494,"user_tz":240,"elapsed":110,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"33fc2a2e-e21e-42e7-d6f2-809d23a24f97"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6875"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# **Neural embedding approach**"],"metadata":{"id":"8kolKJLZ3j6g"}},{"cell_type":"code","source":["import string\n","import re\n","import nltk\n","from nltk import FreqDist, bigrams\n","from gensim.models import Word2Vec\n","from gensim.models.keyedvectors import KeyedVectors\n","import math\n","\n","def preprocess_text(file_path):\n","    with open(file_path, 'r') as file:\n","        letters = file.read().lower()\n","        letters = letters.translate(str.maketrans('', '', string.punctuation))\n","    return letters\n","\n","def calculate_neural_embedding_perplexity(model, tokens):\n","    num_words = len(tokens)\n","    log_sum = 0\n","\n","    for i in range(num_words - 1):\n","        current_word = tokens[i]\n","        next_word = tokens[i + 1]\n","\n","        if current_word in model and next_word in model:\n","            similarity = model.similarity(current_word, next_word)\n","            if similarity > 0:\n","                log_sum += math.log2(similarity)\n","\n","    perplexity = 2 ** (-log_sum / (num_words - 1))\n","    return perplexity\n","\n","# Preprocess the Warren Buffet's letters\n","file_path = '/content/drive/My Drive/WarrenBuffet.txt'\n","letters = preprocess_text(file_path)\n","\n","# Tokenize the text into words\n","words = nltk.word_tokenize(letters)\n","\n","# Train Word2Vec model on the text corpus\n","model = Word2Vec([words], vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Save and load the model to get the KeyedVectors instance\n","model.wv.save_word2vec_format(\"word2vec_model.bin\", binary=True)\n","word_vectors = KeyedVectors.load_word2vec_format(\"word2vec_model.bin\", binary=True)\n","\n","# Calculate the perplexity using the neural embedding approach\n","ne_perplexity = calculate_neural_embedding_perplexity(word_vectors, words)\n","print(f\"Neural Embedding Approach Perplexity: {ne_perplexity}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z6NxYo57c4e","executionInfo":{"status":"ok","timestamp":1688423545134,"user_tz":240,"elapsed":1204,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"be1a665b-ad04-4614-c93a-b3dffdf2b301"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Neural Embedding Approach Perplexity: 2.209746106510776\n"]}]},{"cell_type":"code","source":["if rf_preplexity < ne_perplexity:\n","    print(\"Relative Frequency Approach has lower perplexity.\")\n","elif rf_preplexity > ne_perplexity:\n","    print(\"Neural Embedding Approach has lower perplexity.\")\n","else:\n","    print(\"Both approaches have the same perplexity.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkTwbO3M4Jxg","executionInfo":{"status":"ok","timestamp":1688423574743,"user_tz":240,"elapsed":109,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"8fd723d4-6d83-4195-d910-7594776d06ee"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Neural Embedding Approach has lower perplexity.\n"]}]},{"cell_type":"markdown","source":["The Neural Embedding Approach has lower preplexity compared to the Relative frequency approach."],"metadata":{"id":"QkgzD1i138-1"}}]}